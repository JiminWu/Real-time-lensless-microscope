{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0631597c-f1a9-476a-bc61-e1f1f1b6ecd2",
   "metadata": {},
   "source": [
    "<font size=\"+2\">\n",
    "Reconstruction using pre-tained model\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3f02e-57bb-46a7-b7c9-b8e25d1931fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, cv2, time, hdf5storage\n",
    "\n",
    "import models.dataset as ds\n",
    "import helper as hp\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c13f15-14b5-4760-b575-094fefa7d686",
   "metadata": {},
   "source": [
    "<font size=\"+1\">\n",
    "Load PSFs and pre-tained model\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c7cc7-fd5b-4586-b9b3-8945c7fafcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "psf_file='./trained_data/psf5_2048.mat'\n",
    "model_filepath='./trained_data/'\n",
    "model, args = hp.load_model(psf_file, model_filepath, device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a53af5-8bff-4e33-b182-811d26781748",
   "metadata": {},
   "source": [
    "<font size=\"+1\">\n",
    "Cropping parameters\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade7389-15e0-49ac-80ae-9b4e7f99533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height_org = 2048\n",
    "image_width_org = 2048\n",
    "image_height_crop = 1536\n",
    "image_width_crop = 1536\n",
    "image_shift_height = -56\n",
    "image_shift_width = -56\n",
    "\n",
    "target_height = 768\n",
    "target_width = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22fdd6-0613-4b55-a0a1-9dbfd1a2d5bb",
   "metadata": {},
   "source": [
    "<font size=\"+1\">\n",
    "Load measurements\n",
    "</font>\n",
    "\n",
    "Simulated measurements are .png files, real measurements are .mat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54fa4f3-7f36-40c9-ade2-f901fc08579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_type = 0 # 0 for simulated measurement, 1 for real measurement\n",
    "\n",
    "if measurement_type == 0: # simulated measurement\n",
    "    loaded_meas = ('./captures/simulated/0001.png')\n",
    "    meas_loaded = cv2.imread(loaded_meas)/255.0\n",
    "    meas=meas_loaded[:,:,1]\n",
    "    \n",
    "else: # real meaturement\n",
    "    loaded_meas = ('./captures/real/convallaria_cap_1.56mm.mat')\n",
    "    meas_loaded = hdf5storage.loadmat(loaded_meas)['avgCap']\n",
    "    meas=meas_loaded[:,513:2560]\n",
    "\n",
    "measurement = hp.crop_images(meas, image_height_org, image_width_org, image_height_crop, image_width_crop, image_shift_height, image_shift_width)\n",
    "\n",
    "#meas[crop_x_start:crop_x_end,crop_y_start:crop_y_end]\n",
    "\n",
    "plt.imshow(measurement, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d79d3fb-cb5c-45d9-8028-c495dbbcb54c",
   "metadata": {},
   "source": [
    "<font size=\"+1\">\n",
    "Run reconstruction\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b95c2f-ebda-41c7-8f0d-d9e27c4fea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_tensor=torch.tensor(measurement, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)\n",
    "out = model((measurement_tensor).to(device))\n",
    "output_numpy = out.detach().cpu().numpy()[0][0]\n",
    "del out\n",
    "\n",
    "output_numpy = hp.crop_out_single(output_numpy, target_height, target_width, image_shift_height, image_shift_width)\n",
    "output_numpy[output_numpy <= 0] = 0\n",
    "output_numpy = (output_numpy-output_numpy.min())/(output_numpy.max()-output_numpy.min())\n",
    "\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4400a0ce-54ff-46e9-b0f0-c30b96708e03",
   "metadata": {},
   "source": [
    "<font size=\"+1\">\n",
    "Display & save results\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316688f-924c-45e0-8aed-17ab0fbb76a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_numpy,cmap='gray')\n",
    "\n",
    "out_dir = './recon_net/'\n",
    "outname = [out_dir, '0001.png']\n",
    "plt.imsave(\"\".join(outname), output_numpy, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083ee5a-5295-429a-a263-19a5c2d6c9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiwiener",
   "language": "python",
   "name": "multiwiener"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
